{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Sequence Trading System - Google Colab Quickstart\n",
    "\n",
    "This notebook provides a complete setup and workflow for running the Sequence trading system on Google Colab.\n",
    "\n",
    "**Features:**\n",
    "- CNN-LSTM-Attention hybrid models for FX prediction\n",
    "- Intrinsic time representations (directional-change bars)\n",
    "- GDELT sentiment analysis integration\n",
    "- Reinforcement learning (A3C) for trading policies\n",
    "- Fast NPY data format (30-50x faster than CSV)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## üöÄ Step 1: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the Sequence repository\n",
    "!git clone https://github.com/YOUR_USERNAME/Sequence.git\n",
    "%cd Sequence\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install TimesFM foundation model (optional, for ensemble evaluation)\n",
    "!pip install -q -e ./models/timesFM\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "path-setup-header"
   },
   "source": [
    "## ‚öôÔ∏è Step 2: Setup Python Path (CRITICAL)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: Run this cell in every new Colab session before importing any Sequence modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-paths"
   },
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\n# Add Sequence root and run/ to Python path\nROOT = Path.cwd()\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\nif str(ROOT / \"run\") not in sys.path:\n    sys.path.insert(0, str(ROOT / \"run\"))\n\nprint(\"‚úÖ Python path configured:\")\nprint(f\"   - ROOT: {ROOT}\")\nprint(f\"   - run/: {ROOT / 'run'}\")\n\n# Check NumPy compatibility (common Colab issue)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Checking NumPy compatibility...\")\nprint(\"=\"*60)\n\ntry:\n    import numpy as np\n    # Try importing a compiled extension to test compatibility\n    from numpy.random import RandomState\n    print(\"‚úÖ NumPy compatibility check passed\")\nexcept ValueError as e:\n    if \"numpy.dtype size changed\" in str(e):\n        print(\"‚ö†Ô∏è  NumPy binary incompatibility detected!\")\n        print(\"   This is a common Colab issue when packages were compiled\")\n        print(\"   against a different NumPy version.\\n\")\n        print(\"üîß SOLUTION:\")\n        print(\"   1. Run in a new cell: !pip install --upgrade numpy pandas scikit-learn scipy --quiet\")\n        print(\"   2. Restart runtime: Runtime ‚Üí Restart runtime\")\n        print(\"   3. Re-run this cell\\n\")\n        print(\"‚ùå Cannot continue until NumPy is fixed.\")\n        raise SystemExit(\"NumPy compatibility issue - see instructions above\")\n    else:\n        raise\nexcept ImportError:\n    print(\"‚ö†Ô∏è  NumPy not installed. Run: !pip install -r requirements.txt\")\n\n# Verify imports work\nprint(\"\\n\" + \"=\"*60)\nprint(\"Verifying Sequence imports...\")\nprint(\"=\"*60)\n\ntry:\n    from config.config import DataConfig, ModelConfig, TrainingConfig\n    from train.features.agent_features import build_feature_frame\n    from utils.logger import get_logger\n    print(\"‚úÖ All imports successful!\")\nexcept ImportError as e:\n    print(f\"‚ùå Import failed: {e}\")\n    print(\"Please restart the runtime and try again\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-check-header"
   },
   "source": [
    "## üéÆ Step 3: Check GPU Availability\n",
    "\n",
    "To enable GPU: **Runtime** ‚Üí **Change runtime type** ‚Üí **GPU** (T4 or better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected\")\n",
    "    print(\"   Consider enabling GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drive-header"
   },
   "source": [
    "## üíæ Step 4: Mount Google Drive (Optional - Recommended for Data Persistence)\n",
    "\n",
    "Mount Google Drive to save data and models across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create Sequence data directory in Drive\n",
    "drive_data_dir = Path('/content/drive/MyDrive/Sequence/data')\n",
    "drive_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set environment variable for data directory\n",
    "os.environ['SEQUENCE_DATA_DIR'] = str(drive_data_dir)\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted\")\n",
    "print(f\"   Data directory: {drive_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow-header"
   },
   "source": [
    "---\n",
    "\n",
    "# üìä Workflow Examples\n",
    "\n",
    "Choose a workflow below based on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow1-header"
   },
   "source": [
    "## Workflow 1: Data Preparation Only\n",
    "\n",
    "Prepare dataset with technical features and save in NPY format (30-50x faster loading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow1-prepare"
   },
   "outputs": [],
   "source": [
    "# Prepare GBPUSD data for 2023-2024\n",
    "!python data/prepare_dataset.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --years 2023,2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10 \\\n",
    "  --task-type classification \\\n",
    "  --input-root data/data\n",
    "\n",
    "# Output files created:\n",
    "# - data/data/gbpusd/gbpusd_prepared.npy (fast binary format)\n",
    "# - data/data/gbpusd/gbpusd_prepared.csv (backward compatibility)\n",
    "# - data/data/gbpusd/gbpusd_prepared_metadata.json (feature info)\n",
    "# - data/data/gbpusd/gbpusd_prepared_datetime.npy (datetime index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow2-header"
   },
   "source": [
    "## Workflow 2: Complete Training Pipeline\n",
    "\n",
    "Download data, prepare features, and train model end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow2-full-pipeline"
   },
   "outputs": [],
   "source": [
    "# Complete pipeline: download ‚Üí prepare ‚Üí train\n",
    "!python run/training_pipeline.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --run-histdata-download \\\n",
    "  --years 2023,2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10 \\\n",
    "  --epochs 20 \\\n",
    "  --batch-size 64 \\\n",
    "  --learning-rate 1e-3 \\\n",
    "  --checkpoint-dir models\n",
    "\n",
    "# Model checkpoint will be saved to: models/gbpusd_best_model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow3-header"
   },
   "source": [
    "## Workflow 3: Training with Intrinsic Time (Directional-Change Bars)\n",
    "\n",
    "Use event-driven time representation instead of fixed intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow3-intrinsic-time"
   },
   "outputs": [],
   "source": [
    "# Prepare data with directional-change bars\n",
    "!python data/prepare_dataset.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --years 2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10 \\\n",
    "  --intrinsic-time \\\n",
    "  --dc-threshold-up 0.0005 \\\n",
    "  --dc-threshold-down 0.0005\n",
    "\n",
    "# Train on intrinsic time data\n",
    "!python train/run_training.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --epochs 20 \\\n",
    "  --batch-size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow4-header"
   },
   "source": [
    "## Workflow 4: Multi-Task Learning (Price + Volatility + Regime)\n",
    "\n",
    "Train model to predict multiple targets simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow4-multitask"
   },
   "outputs": [],
   "source": [
    "# Prepare multi-task dataset\n",
    "!python data/prepare_multitask_dataset.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --years 2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10\n",
    "\n",
    "# Train multi-task model\n",
    "!python train/run_training_multitask.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --epochs 30 \\\n",
    "  --batch-size 64 \\\n",
    "  --learning-rate 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow5-header"
   },
   "source": [
    "## Workflow 5: GDELT Sentiment Analysis Integration (BigQuery)\n",
    "\n",
    "Add news sentiment features using Google BigQuery GDELT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow5-auth"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "import os\n",
    "\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Set your GCP project ID\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = 'your-project-id'  # Replace with your project ID\n",
    "\n",
    "print(\"‚úÖ Google Cloud authentication complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow5-gdelt"
   },
   "outputs": [],
   "source": [
    "# Prepare data with GDELT sentiment features\n",
    "!python data/prepare_dataset.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --years 2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10 \\\n",
    "  --include-sentiment \\\n",
    "  --use-bigquery-gdelt \\\n",
    "  --gdelt-themes \"WB_1427_BUSINESS_FINANCE,WB_2327_BUSINESS_FINANCIAL_MARKETS\"\n",
    "\n",
    "# Train with sentiment features\n",
    "!python train/run_training.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --epochs 20 \\\n",
    "  --batch-size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workflow6-header"
   },
   "source": [
    "## Workflow 6: Reinforcement Learning (A3C) with Backtesting\n",
    "\n",
    "Train an RL agent to learn optimal trading policies on historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow6-rl"
   },
   "outputs": [],
   "source": [
    "# First, prepare data if not already done\n",
    "!python data/prepare_dataset.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --years 2023,2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10\n",
    "\n",
    "# Train A3C agent with backtesting environment\n",
    "!python rl/run_a3c_training.py \\\n",
    "  --pair gbpusd \\\n",
    "  --env-mode backtesting \\\n",
    "  --historical-data data/data/gbpusd/gbpusd_prepared.csv \\\n",
    "  --num-workers 2 \\\n",
    "  --total-steps 100000 \\\n",
    "  --learning-rate 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval-header"
   },
   "source": [
    "---\n",
    "\n",
    "# üìà Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval-basic-header"
   },
   "source": [
    "## Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval-basic"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "!python eval/run_evaluation.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --checkpoint-path models/gbpusd_best_model.pt \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10\n",
    "\n",
    "# Outputs: accuracy, precision, recall, F1 score, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval-ensemble-header"
   },
   "source": [
    "## Ensemble with TimesFM Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval-ensemble"
   },
   "outputs": [],
   "source": [
    "# Ensemble your model with Google's TimesFM\n",
    "!python eval/ensemble_timesfm.py \\\n",
    "  --pairs gbpusd \\\n",
    "  --years 2024 \\\n",
    "  --t-in 120 \\\n",
    "  --t-out 10 \\\n",
    "  --checkpoint-root models \\\n",
    "  --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utils-header"
   },
   "source": [
    "---\n",
    "\n",
    "# üõ†Ô∏è Utility Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-data-header"
   },
   "source": [
    "## Check Prepared Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-data"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "pair = \"gbpusd\"\n",
    "data_dir = Path(f\"data/data/{pair}\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    print(f\"üìÇ Data files for {pair.upper()}:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for file_path in sorted(data_dir.glob(f\"{pair}_prepared*\")):\n",
    "        size_mb = file_path.stat().st_size / 1e6\n",
    "        print(f\"  {file_path.name:40} {size_mb:>8.2f} MB\")\n",
    "\n",
    "    # Show metadata\n",
    "    metadata_path = data_dir / f\"{pair}_prepared_metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path) as f:\n",
    "            metadata = json.load(f)\n",
    "        print(\"\\nüìä Dataset Info:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Rows: {metadata['num_rows']:,}\")\n",
    "        print(f\"  Features: {len(metadata['feature_columns'])}\")\n",
    "        print(f\"  t_in: {metadata['t_in']}, t_out: {metadata['t_out']}\")\n",
    "        print(f\"  Target type: {metadata['target_type']}\")\n",
    "else:\n",
    "    print(f\"‚ùå No data found for {pair.upper()}\")\n",
    "    print(\"   Run data preparation first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-models-header"
   },
   "source": [
    "## Check Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-models"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "\n",
    "if models_dir.exists():\n",
    "    model_files = list(models_dir.glob(\"*.pt\"))\n",
    "\n",
    "    if model_files:\n",
    "        print(\"ü§ñ Saved Models:\")\n",
    "        print(\"=\"*60)\n",
    "        for model_path in sorted(model_files):\n",
    "            size_mb = model_path.stat().st_size / 1e6\n",
    "            print(f\"  {model_path.name:40} {size_mb:>8.2f} MB\")\n",
    "    else:\n",
    "        print(\"‚ùå No model checkpoints found\")\n",
    "        print(\"   Train a model first\")\n",
    "else:\n",
    "    print(\"‚ùå models/ directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor-gpu-header"
   },
   "source": [
    "## Monitor GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting-header"
   },
   "source": [
    "---\n",
    "\n",
    "# üîß Troubleshooting\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### Import Errors\n",
    "**Error**: `ModuleNotFoundError: No module named 'config'`\n",
    "\n",
    "**Solution**: Re-run Step 2 (Setup Python Path) cell\n",
    "\n",
    "### Out of Memory\n",
    "**Error**: `CUDA out of memory`\n",
    "\n",
    "**Solution**: Reduce batch size:\n",
    "```bash\n",
    "--batch-size 32  # or even 16\n",
    "```\n",
    "\n",
    "### Slow Data Loading\n",
    "**Issue**: Loading takes 30-60 seconds\n",
    "\n",
    "**Solution**: The codebase now automatically uses NPY format (30-50x faster). Make sure you're using the latest version.\n",
    "\n",
    "### Missing Data Files\n",
    "**Error**: `FileNotFoundError: gbpusd_prepared.csv`\n",
    "\n",
    "**Solution**: Run data preparation first (Workflow 1)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **Main Documentation**: [README.md](https://github.com/YOUR_USERNAME/Sequence/blob/main/README.md)\n",
    "- **Colab Setup Guide**: [COLAB_SETUP.md](https://github.com/YOUR_USERNAME/Sequence/blob/main/COLAB_SETUP.md)\n",
    "- **Configuration Reference**: [CLAUDE.md](https://github.com/YOUR_USERNAME/Sequence/blob/main/CLAUDE.md)\n",
    "- **Architecture Guide**: [docs/ARCHITECTURE_OVERVIEW.md](https://github.com/YOUR_USERNAME/Sequence/blob/main/docs/ARCHITECTURE_OVERVIEW.md)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Trading! üöÄüìà**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}