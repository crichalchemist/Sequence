{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Training Pipeline - Production Workflow\n",
    "\n",
    "This notebook provides a complete, production-ready training workflow for the Sequence framework.\n",
    "\n",
    "**Features:**\n",
    "- Requirements installation\n",
    "- Multi-pair sequential training\n",
    "- Real data loading from prepared CSV files\n",
    "- Train/validation/test splitting\n",
    "- Multi-GPU training support\n",
    "- Automatic Mixed Precision (AMP)\n",
    "- Early stopping and learning rate scheduling\n",
    "- Supervised learning (classification)\n",
    "- Reinforcement learning (SAC)\n",
    "- Checkpoint management\n",
    "- Comprehensive evaluation and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1009031037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Mount Google Drive (if using Google Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ“ Google Drive mounted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "#Mount Google Drive (if using Google Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ“ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Repository Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "zip_path = Path('/content/drive/MyDrive/Sequence.zip')\n",
    "extract_to = Path('/content/Sequence')\n",
    "\n",
    "if not zip_path.exists():\n",
    "    print(f\"âŒ ERROR: {zip_path} not found\")\n",
    "    print(\"Upload Sequence.zip to your Google Drive root (MyDrive)\")\n",
    "else:\n",
    "    print(f\"Extracting {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content')\n",
    "\n",
    "    # Handle possible directory naming\n",
    "    extracted = [d for d in Path('/content').iterdir() if d.is_dir() and 'Sequence' in d.name]\n",
    "    if extracted and extracted[0] != extract_to:\n",
    "        extracted[0].rename(extract_to)\n",
    "\n",
    "    print(\"âœ“ Repository extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fix Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# UPDATED FOR COLAB\n",
    "ROOT = Path('/content/Sequence')  # Changed from /Volumes/Containers/Sequence\n",
    "sys.path.insert(0, str(ROOT))\n",
    "sys.path.insert(0, str(ROOT / 'run'))\n",
    "\n",
    "# Create necessary directories\n",
    "(ROOT / 'data' / 'data').mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / 'data' / 'raw').mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / 'models' / 'checkpoints').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Root: {ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn tqdm\n",
    "!pip install -q transformers backtesting ta histdata\n",
    "\n",
    "print(\"âœ“ All requirements installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **CONFIGURATION** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTI-PAIR CONFIGURATION\n",
    "# =============================================================================\n",
    "# Training will run sequentially for each pair in the list\n",
    "PAIRS = [\n",
    "    \"eurusd\",\"eurgbp\",\"eurjpy\",\"eurchf\",\n",
    "    \"euraud\",\"eurcad\",\"eurnzd\",\"gbpusd\", \n",
    "    \"gbpjpy\",\"gbpchf\",\"gbpcad\",\"gbpaud\",\n",
    "    \"gbpnzd\",\"usdjpy\",\"usdchf\",\"usdcad\",\n",
    "    \"audusd\",\"audjpy\",\"audcad\",\"audchf\",\n",
    "    \"audnzd\",\"nzdusd\",\"nzdjpy\",\"nzdcad\",\n",
    "    \"nzdchf\",\"cadchf\",\"cadjpy\",\"chfjpy\",\n",
    "    \"usdbrl\",\"usdrub\",\"usdinr\",\"usdcny\",\n",
    "    \"usdzar\",\"usdtry\"\n",
    "]  # Add more pairs as needed\n",
    "\n",
    "# Training mode: 'supervised' or 'sac'\n",
    "TRAINING_MODE = \"supervised\"  # Options: 'supervised', 'sac'\n",
    "\n",
    "# Data configuration\n",
    "T_IN = 120  # Input sequence length\n",
    "T_OUT = 10  # Prediction horizon\n",
    "\n",
    "# =============================================================================\n",
    "# SUPERVISED LEARNING CONFIGURATION\n",
    "# =============================================================================\n",
    "SUPERVISED_CONFIG = {\n",
    "    # Model architecture\n",
    "    \"hidden_size_lstm\": 128,\n",
    "    \"num_layers_lstm\": 2,\n",
    "    \"cnn_num_filters\": 64,\n",
    "    \"attention_dim\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 3,  # 3 for classification (down/neutral/up)\n",
    "    # Training hyperparameters\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_amp\": False,\n",
    "    # Data splitting\n",
    "    \"train_ratio\": 0.7,\n",
    "    \"val_ratio\": 0.15,\n",
    "    \"test_ratio\": 0.15,\n",
    "    # Checkpointing\n",
    "    \"checkpoint_dir\": \"models/checkpoints\",\n",
    "    \"early_stop_patience\": 5,\n",
    "    \"lr_scheduler_patience\": 3,\n",
    "    \"lr_scheduler_factor\": 0.5,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# SAC (REINFORCEMENT LEARNING) CONFIGURATION\n",
    "# =============================================================================\n",
    "SAC_CONFIG = {\n",
    "    # Agent hyperparameters\n",
    "    \"hidden_dim\": 256,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"gamma\": 0.99,  # Discount factor\n",
    "    \"tau\": 0.005,  # Soft update coefficient\n",
    "    \"alpha\": 0.2,  # Entropy temperature (if not auto-tuning)\n",
    "    \"auto_entropy_tuning\": True,\n",
    "    # Training configuration\n",
    "    \"total_steps\": 100000,\n",
    "    \"batch_size\": 256,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"warmup_steps\": 1000,  # Random exploration steps\n",
    "    \"update_interval\": 1,  # Update every N steps\n",
    "    \"eval_interval\": 5000,  # Evaluate every N steps\n",
    "    \"eval_episodes\": 10,\n",
    "    # Environment configuration\n",
    "    \"initial_cash\": 50000.0,\n",
    "    \"time_horizon\": 100,\n",
    "    \"commission_pct\": 0.0001,  # 0.01%\n",
    "    \"spread_bps\": 1.0,  # 1 basis point\n",
    "    \"reward_type\": \"incremental_pnl\",  # Options: 'portfolio_value', 'incremental_pnl', 'sharpe', 'cost_aware'\n",
    "    \"reward_scaling\": 1.0,\n",
    "    # Checkpointing\n",
    "    \"checkpoint_dir\": \"models/sac_checkpoints\",\n",
    "    \"save_interval\": 10000,\n",
    "}\n",
    "\n",
    "print(f\"Training mode: {TRAINING_MODE.upper()}\")\n",
    "print(f\"Pairs to train: {PAIRS}\")\n",
    "print(f\"Total pairs: {len(PAIRS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ***GPU CHECK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"=\"*60)\n",
    "print(\"GPU Environment Check\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(\"\\nGPU Details:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    Compute Capability: {props.major}.{props.minor}\")\n",
    "    \n",
    "    print(\"\\nCurrent GPU Utilization:\")\n",
    "    print_gpu_utilization()\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No GPU detected. Training will use CPU.\")\n",
    "    print(\"For best performance, run on a GPU-enabled environment.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **DATA COLLECTION FROM HISTDATA** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from histdata.api import download_hist_data\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration (update these as needed)\n",
    "PAIRS_TO_DOWNLOAD = [\"eurusd\",\"eurgbp\",\"eurjpy\",\"eurchf\",\n",
    "    \"euraud\",\"eurcad\",\"eurnzd\",\"gbpusd\", \n",
    "    \"gbpjpy\",\"gbpchf\",\"gbpcad\",\"gbpaud\",\n",
    "    \"gbpnzd\",\"usdjpy\",\"usdchf\",\"usdcad\",\n",
    "    \"audusd\",\"audjpy\",\"audcad\",\"audchf\",\n",
    "    \"audnzd\",\"nzdusd\",\"nzdjpy\",\"nzdcad\",\n",
    "    \"nzdchf\",\"cadchf\",\"cadjpy\",\"chfjpy\",\n",
    "    \"usdbrl\",\"usdrub\",\"usdinr\",\"usdcny\",\n",
    "    \"usdzar\",\"usdtry\"]  # Start with 2-3 pairs\n",
    "YEARS_TO_DOWNLOAD = ['2010','2011','2012','2013','2014','2015',\n",
    "                     '2016','2017','2018','2019','2020','2021','2022','2023','2024','2025']\n",
    "print(\"ðŸ“¥ Downloading Historical FX Data from HistData.com\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Pairs: {', '.join([p.upper() for p in PAIRS_TO_DOWNLOAD])}\")\n",
    "print(f\"Years: {', '.join(YEARS_TO_DOWNLOAD)}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "raw_data_dir = ROOT / 'data' / 'raw'\n",
    "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_stats = {'successful': 0, 'failed': 0}\n",
    "\n",
    "for pair in PAIRS_TO_DOWNLOAD:\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"ðŸ“Š Pair: {pair.upper()}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "\n",
    "    pair_dir = raw_data_dir / pair\n",
    "    pair_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for year in YEARS_TO_DOWNLOAD:\n",
    "        try:\n",
    "            print(f\"  {year}... \", end='', flush=True)\n",
    "\n",
    "            # Try downloading full year first\n",
    "            try:\n",
    "                result = download_hist_data(\n",
    "                    year=int(year),\n",
    "                    pair=pair,\n",
    "                    output_directory=str(pair_dir),\n",
    "                    verbose=False\n",
    "                )\n",
    "                print(f\"âœ“ {result}\")\n",
    "                download_stats['successful'] += 1\n",
    "\n",
    "            except AssertionError:\n",
    "                # Download month-by-month if full year not available\n",
    "                print(\"(month-by-month)\")\n",
    "                months_ok = 0\n",
    "                for month in range(1, 13):\n",
    "                    try:\n",
    "                        download_hist_data(\n",
    "                            year=int(year),\n",
    "                            month=month,\n",
    "                            pair=pair,\n",
    "                            output_directory=str(pair_dir),\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        months_ok += 1\n",
    "                        download_stats['successful'] += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                print(f\"    âœ“ {months_ok} months downloaded\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {str(e)[:50]}\")\n",
    "            download_stats['failed'] += 1\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ Successful: {download_stats['successful']}\")\n",
    "print(f\"âœ— Failed: {download_stats['failed']}\")\n",
    "print(f\"âœ“ Data collection complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ***DATA PREPARATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Starting data preparation pipeline...\\\\n\")\n",
    "\n",
    "for pair in PAIRS_TO_DOWNLOAD:  # Changed from PAIRS_TO_COLLECT\n",
    "    print(f\"\\\\nProcessing {pair.upper()}...\")\n",
    "\n",
    "    cmd = [\n",
    "        'python', str(ROOT / 'data' / 'prepare_dataset.py'),\n",
    "        '--pairs', pair,\n",
    "        '--t-in', str(T_IN),\n",
    "        '--t-out', str(T_OUT),\n",
    "        '--task-type', TASK_TYPE,\n",
    "    ]\n",
    "\n",
    "    # Add optional flags\n",
    "    if USE_INTRINSIC_TIME:\n",
    "        cmd.append('--intrinsic-time')\n",
    "    if INCLUDE_SENTIMENT:\n",
    "        cmd.append('--include-sentiment')\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(ROOT))\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  âœ“ {pair.upper()} prepared\")\n",
    "    else:\n",
    "        print(f\"  âœ— Failed: {result.stderr}\")\n",
    "\n",
    "print(\"\\\\nâœ“ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ***SETUP AND IMPORTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT))\n",
    "sys.path.insert(0, str(ROOT / 'run'))\n",
    "\n",
    "print(f\"Project root: {ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Project imports\n",
    "from config.config import (\n",
    "    AssetConfig, AssetClass, DataConfig, \n",
    "    FeatureConfig, ModelConfig, TrainingConfig\n",
    ")\n",
    "from utils.multi_gpu import setup_multi_gpu, print_gpu_utilization, get_gpu_memory_info\n",
    "from train.features.agent_features import build_feature_frame\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unwrapped_model(model):\n",
    "    \"\"\"Get the underlying model from DataParallel wrapper.\"\"\"\n",
    "    return model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into human-readable time.\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    \n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        return f\"{secs}s\"\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets, t_in):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.LongTensor(targets) if targets.dtype == np.int64 else torch.FloatTensor(targets)\n",
    "        self.t_in = t_in\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.t_in\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx + self.t_in]\n",
    "        y = self.targets[idx + self.t_in]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ***Supervised Learning Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_supervised(model, loader, criterion, optimizer, device, scaler=None, grad_clip=None):\n",
    "    \"\"\"Train for one epoch (supervised learning).\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate_supervised(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model (supervised learning).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def train_supervised(pair, config):\n",
    "    \"\"\"Complete supervised training pipeline for a single pair.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SUPERVISED TRAINING: {pair.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    data_path = ROOT / 'data' / 'data' / pair / f'{pair}_prepared.csv'\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"âœ“ Loaded {len(df):,} rows\\n\")\n",
    "    \n",
    "    # Separate features and targets\n",
    "    target_col = 'target' if 'target' in df.columns else 'label'\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'timestamp', 'date', 'time']]\n",
    "    features = df[feature_cols].values\n",
    "    targets = df[target_col].values\n",
    "    \n",
    "    # Split data\n",
    "    n_samples = len(features)\n",
    "    train_end = int(n_samples * config['train_ratio'])\n",
    "    val_end = int(n_samples * (config['train_ratio'] + config['val_ratio']))\n",
    "    \n",
    "    train_features, train_targets = features[:train_end], targets[:train_end]\n",
    "    val_features, val_targets = features[train_end:val_end], targets[train_end:val_end]\n",
    "    test_features, test_targets = features[val_end:], targets[val_end:]\n",
    "    \n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"  Train: {len(train_features):,} samples ({config['train_ratio']*100:.0f}%)\")\n",
    "    print(f\"  Val:   {len(val_features):,} samples ({config['val_ratio']*100:.0f}%)\")\n",
    "    print(f\"  Test:  {len(test_features):,} samples ({config['test_ratio']*100:.0f}%)\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SequenceDataset(train_features, train_targets, T_IN)\n",
    "    val_dataset = SequenceDataset(val_features, val_targets, T_IN)\n",
    "    test_dataset = SequenceDataset(test_features, test_targets, T_IN)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Create model\n",
    "    model_config = ModelConfig(\n",
    "        num_features=len(feature_cols),\n",
    "        hidden_size_lstm=config['hidden_size_lstm'],\n",
    "        num_layers_lstm=config['num_layers_lstm'],\n",
    "        cnn_num_filters=config['cnn_num_filters'],\n",
    "        attention_dim=config['attention_dim'],\n",
    "        dropout=config['dropout'],\n",
    "        num_classes=config['num_classes'],\n",
    "        use_optimized_attention=False\n",
    "    )\n",
    "    \n",
    "    model = HybridCNNLSTMAttention(model_config)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, device = setup_multi_gpu(model, device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: HybridCNNLSTMAttention ({total_params:,} parameters)\")\n",
    "    print(f\"Device: {device}\\n\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=config['lr_scheduler_factor'], \n",
    "        patience=config['lr_scheduler_patience'], verbose=False\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler() if config['use_amp'] and torch.cuda.is_available() else None\n",
    "    \n",
    "    # Checkpoint directory\n",
    "    checkpoint_dir = ROOT / config['checkpoint_dir']\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f'{pair}_best_model.pt'\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\\n\")\n",
    "    \n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_acc = train_epoch_supervised(\n",
    "            model, train_loader, criterion, optimizer, device, scaler, config['grad_clip']\n",
    "        )\n",
    "        val_loss, val_acc = validate_supervised(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{config['epochs']} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% | \"\n",
    "                  f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': get_unwrapped_model(model).state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'model_config': model_config.__dict__,\n",
    "                'pair': pair,\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= config['early_stop_patience']:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Test evaluation\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    get_unwrapped_model(model).load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_loss, test_acc = validate_supervised(model, test_loader, criterion, device)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING COMPLETE: {pair.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Time elapsed: {format_time(elapsed_time)}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.4f} (epoch {checkpoint['epoch']})\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Checkpoint: {checkpoint_path}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'pair': pair,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'epochs_trained': len(history['train_loss']),\n",
    "        'time_elapsed': elapsed_time,\n",
    "        'checkpoint_path': str(checkpoint_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ***SAC (Reinforcement Learning) Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sac_agent(agent, env, num_episodes=10):\n",
    "    \"\"\"Evaluate SAC agent over multiple episodes.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get state vector from observation\n",
    "            state = np.array([obs['portfolio_value'], obs['position'], obs['cash']])\n",
    "            action = agent.select_action(state, evaluate=True)\n",
    "            \n",
    "            # Convert continuous action to OrderAction\n",
    "            size = abs(action[0]) * 10.0  # Scale action\n",
    "            side = 'buy' if action[0] > 0 else 'sell'\n",
    "            order = OrderAction(action_type='market', side=side, size=size)\n",
    "            \n",
    "            obs, reward, done, info = env.step(order)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(episode_rewards),\n",
    "        'std_reward': np.std(episode_rewards),\n",
    "        'mean_length': np.mean(episode_lengths)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_sac(pair, config):\n",
    "    \"\"\"Complete SAC training pipeline for a single pair.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAC TRAINING: {pair.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data for environment\n",
    "    data_path = ROOT / 'data' / 'data' / pair / f'{pair}_prepared.csv'\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"âœ“ Loaded {len(df):,} rows\\n\")\n",
    "    \n",
    "    # Create execution environment\n",
    "    exec_config = ExecutionConfig(\n",
    "        initial_cash=config['initial_cash'],\n",
    "        time_horizon=config['time_horizon'],\n",
    "        commission_pct=config['commission_pct'],\n",
    "        spread_bps=config['spread_bps'],\n",
    "        reward_type=config['reward_type'],\n",
    "        reward_scaling=config['reward_scaling']\n",
    "    )\n",
    "    \n",
    "    env = SimulatedRetailExecutionEnv(exec_config, seed=42)\n",
    "    print(f\"Environment: SimulatedRetailExecutionEnv\")\n",
    "    print(f\"  Initial cash: ${config['initial_cash']:,.2f}\")\n",
    "    print(f\"  Time horizon: {config['time_horizon']} steps\")\n",
    "    print(f\"  Reward type: {config['reward_type']}\\n\")\n",
    "    \n",
    "    # Create SAC agent\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    state_dim = 3  # portfolio_value, position, cash\n",
    "    action_dim = 1  # continuous action (trade size)\n",
    "    \n",
    "    agent = SACAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        lr=config['learning_rate'],\n",
    "        gamma=config['gamma'],\n",
    "        tau=config['tau'],\n",
    "        alpha=config['alpha'],\n",
    "        auto_entropy_tuning=config['auto_entropy_tuning'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"SAC Agent created (device: {device})\")\n",
    "    print(f\"  State dim: {state_dim}\")\n",
    "    print(f\"  Action dim: {action_dim}\")\n",
    "    print(f\"  Hidden dim: {config['hidden_dim']}\")\n",
    "    print(f\"  Auto entropy tuning: {config['auto_entropy_tuning']}\\n\")\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_buffer = ReplayBuffer(capacity=config['replay_buffer_size'])\n",
    "    \n",
    "    # Checkpoint directory\n",
    "    checkpoint_dir = ROOT / config['checkpoint_dir']\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting SAC training for {config['total_steps']:,} steps...\\n\")\n",
    "    \n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    episode_num = 0\n",
    "    \n",
    "    episode_rewards = []\n",
    "    update_metrics = []\n",
    "    \n",
    "    for step in tqdm(range(1, config['total_steps'] + 1), desc=\"Training\"):\n",
    "        # Get state\n",
    "        state = np.array([obs['portfolio_value'], obs['position'], obs['cash']])\n",
    "        \n",
    "        # Select action\n",
    "        if step < config['warmup_steps']:\n",
    "            action = np.random.uniform(-1, 1, size=(action_dim,))\n",
    "        else:\n",
    "            action = agent.select_action(state, evaluate=False)\n",
    "        \n",
    "        # Convert to OrderAction\n",
    "        size = abs(action[0]) * 10.0\n",
    "        side = 'buy' if action[0] > 0 else 'sell'\n",
    "        order = OrderAction(action_type='market', side=side, size=size)\n",
    "        \n",
    "        # Step environment\n",
    "        next_obs, reward, done, info = env.step(order)\n",
    "        next_state = np.array([next_obs['portfolio_value'], next_obs['position'], next_obs['cash']])\n",
    "        \n",
    "        # Store transition\n",
    "        replay_buffer.add(state, action[0], reward, next_state, done)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Update agent\n",
    "        if len(replay_buffer) >= config['batch_size'] and step % config['update_interval'] == 0:\n",
    "            metrics = agent.update(replay_buffer, batch_size=config['batch_size'])\n",
    "            update_metrics.append(metrics)\n",
    "        \n",
    "        # Episode end\n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_num += 1\n",
    "            obs = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "        \n",
    "        # Evaluation\n",
    "        if step % config['eval_interval'] == 0:\n",
    "            eval_results = evaluate_sac_agent(agent, env, num_episodes=config['eval_episodes'])\n",
    "            print(f\"\\nStep {step:6d} | \"\n",
    "                  f\"Episodes: {episode_num} | \"\n",
    "                  f\"Eval reward: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f} | \"\n",
    "                  f\"Alpha: {agent.alpha:.3f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if step % config['save_interval'] == 0:\n",
    "            checkpoint_path = checkpoint_dir / f'{pair}_sac_step_{step}.pt'\n",
    "            agent.save(str(checkpoint_path))\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_eval = evaluate_sac_agent(agent, env, num_episodes=50)\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_checkpoint = checkpoint_dir / f'{pair}_sac_final.pt'\n",
    "    agent.save(str(final_checkpoint))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAC TRAINING COMPLETE: {pair.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Time elapsed: {format_time(elapsed_time)}\")\n",
    "    print(f\"Total episodes: {episode_num}\")\n",
    "    print(f\"Final eval reward: {final_eval['mean_reward']:.2f} Â± {final_eval['std_reward']:.2f}\")\n",
    "    print(f\"Checkpoint: {final_checkpoint}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'pair': pair,\n",
    "        'total_episodes': episode_num,\n",
    "        'final_eval_reward_mean': final_eval['mean_reward'],\n",
    "        'final_eval_reward_std': final_eval['std_reward'],\n",
    "        'time_elapsed': elapsed_time,\n",
    "        'checkpoint_path': str(final_checkpoint)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ***Multi-Pair Training Loop***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results for all pairs\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(f\"MULTI-PAIR TRAINING SESSION\")\n",
    "print(f\"{'#'*80}\")\n",
    "print(f\"Mode: {TRAINING_MODE.upper()}\")\n",
    "print(f\"Pairs: {', '.join([p.upper() for p in PAIRS])}\")\n",
    "print(f\"Total pairs: {len(PAIRS)}\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "import time\n",
    "session_start_time = time.time()\n",
    "\n",
    "for idx, pair in enumerate(PAIRS, 1):\n",
    "    print(f\"\\n[{idx}/{len(PAIRS)}] Processing {pair.upper()}...\")\n",
    "    \n",
    "    try:\n",
    "        if TRAINING_MODE == 'supervised':\n",
    "            result = train_supervised(pair, SUPERVISED_CONFIG)\n",
    "        elif TRAINING_MODE == 'sac':\n",
    "            result = train_sac(pair, SAC_CONFIG)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown training mode: {TRAINING_MODE}\")\n",
    "        \n",
    "        all_results.append(result)\n",
    "        print(f\"âœ“ {pair.upper()} completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {pair.upper()} failed: {e}\")\n",
    "        all_results.append({\n",
    "            'pair': pair,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Show remaining pairs\n",
    "    if idx < len(PAIRS):\n",
    "        remaining = PAIRS[idx:]\n",
    "        print(f\"\\nRemaining pairs: {', '.join([p.upper() for p in remaining])}\")\n",
    "\n",
    "session_elapsed_time = time.time() - session_start_time\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(f\"ALL TRAINING COMPLETE\")\n",
    "print(f\"{'#'*80}\")\n",
    "print(f\"Total session time: {format_time(session_elapsed_time)}\")\n",
    "print(f\"Pairs processed: {len(all_results)}/{len(PAIRS)}\")\n",
    "print(f\"{'#'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE == 'supervised':\n",
    "    # Plot test accuracy comparison\n",
    "    successful_results = [r for r in all_results if 'test_acc' in r]\n",
    "    \n",
    "    if successful_results:\n",
    "        pairs = [r['pair'].upper() for r in successful_results]\n",
    "        test_accs = [r['test_acc'] for r in successful_results]\n",
    "        times = [r['time_elapsed'] / 60 for r in successful_results]  # Convert to minutes\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Test accuracy\n",
    "        axes[0].bar(pairs, test_accs, color='steelblue', alpha=0.8)\n",
    "        axes[0].set_ylabel('Test Accuracy (%)')\n",
    "        axes[0].set_title('Test Accuracy by Pair')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[0].axhline(y=50, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Training time\n",
    "        axes[1].bar(pairs, times, color='coral', alpha=0.8)\n",
    "        axes[1].set_ylabel('Training Time (minutes)')\n",
    "        axes[1].set_title('Training Time by Pair')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "elif TRAINING_MODE == 'sac':\n",
    "    # Plot final reward comparison\n",
    "    successful_results = [r for r in all_results if 'final_eval_reward_mean' in r]\n",
    "    \n",
    "    if successful_results:\n",
    "        pairs = [r['pair'].upper() for r in successful_results]\n",
    "        rewards = [r['final_eval_reward_mean'] for r in successful_results]\n",
    "        stds = [r['final_eval_reward_std'] for r in successful_results]\n",
    "        times = [r['time_elapsed'] / 60 for r in successful_results]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Final rewards\n",
    "        axes[0].bar(pairs, rewards, yerr=stds, color='mediumseagreen', alpha=0.8, capsize=5)\n",
    "        axes[0].set_ylabel('Final Eval Reward')\n",
    "        axes[0].set_title('Final Evaluation Reward by Pair')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Training time\n",
    "        axes[1].bar(pairs, times, color='coral', alpha=0.8)\n",
    "        axes[1].set_ylabel('Training Time (minutes)')\n",
    "        axes[1].set_title('Training Time by Pair')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. ***BACKUP TO DRIVE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoints to Google Drive for persistence\n",
    "import shutil\n",
    "\n",
    "drive_checkpoint_dir = Path('/content/drive/MyDrive/Sequence_Models')\n",
    "drive_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Backing up checkpoints to Google Drive...\\\\n\")\n",
    "\n",
    "checkpoint_dir = ROOT / 'models' / 'checkpoints'\n",
    "for checkpoint_file in checkpoint_dir.glob('*.pt'):\n",
    "    shutil.copy2(checkpoint_file, drive_checkpoint_dir / checkpoint_file.name)\n",
    "    print(f\"âœ“ {checkpoint_file.name}\")\n",
    "\n",
    "print(f\"\\\\nâœ“ Checkpoints saved to: {drive_checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
