{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54ea7fa",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c437401",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-920391702.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úì Google Drive mounted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úì Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5a621",
   "metadata": {},
   "source": [
    "## 2. Extract Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "zip_path = Path('/content/drive/MyDrive/Sequence.zip')\n",
    "extract_to = Path('/content/Sequence')\n",
    "\n",
    "if not zip_path.exists():\n",
    "    print(f\"‚ùå ERROR: {zip_path} not found\")\n",
    "    print(\"Upload Sequence.zip to your Google Drive root (MyDrive)\")\n",
    "else:\n",
    "    print(f\"Extracting {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content')\n",
    "    \n",
    "    # Handle possible directory naming\n",
    "    extracted = [d for d in Path('/content').iterdir() if d.is_dir() and 'Sequence' in d.name]\n",
    "    if extracted and extracted[0] != extract_to:\n",
    "        extracted[0].rename(extract_to)\n",
    "    \n",
    "    print(\"‚úì Repository extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6d640",
   "metadata": {},
   "source": [
    "## 3. Setup Paths (Colab-Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d83d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path('/content/Sequence')\n",
    "sys.path.insert(0, str(ROOT))\n",
    "sys.path.insert(0, str(ROOT / 'run'))\n",
    "\n",
    "# Create necessary directories\n",
    "(ROOT / 'data' / 'data').mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / 'data' / 'raw').mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / 'models' / 'checkpoints').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Root: {ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a671e9",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a710004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn tqdm\n",
    "!pip install -q transformers backtesting ta histdata\n",
    "\n",
    "print(\"‚úì All requirements installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671000b",
   "metadata": {},
   "source": [
    "## 5. Configuration (Colab-Optimized)\n",
    "\n",
    "‚ö†Ô∏è **Adjust these settings based on your needs and Colab tier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f80b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GOOGLE COLAB CONFIGURATION\n",
    "# =============================================================================\n",
    "# Start small for free tier limits (T4 GPU has ~15GB RAM)\n",
    "PAIRS_TO_DOWNLOAD = ['gbpusd', 'eurusd']  # Limit to 2-3 pairs initially\n",
    "YEARS_TO_DOWNLOAD = ['2023', '2024']      # 2 years of data\n",
    "\n",
    "# Data settings\n",
    "T_IN = 120\n",
    "T_OUT = 10\n",
    "TASK_TYPE = 'classification'\n",
    "USE_INTRINSIC_TIME = False  # Set True for better performance (slower)\n",
    "INCLUDE_SENTIMENT = False   # Set True to add GDELT sentiment (much slower)\n",
    "\n",
    "# Training mode\n",
    "TRAINING_MODE = \"supervised\"  # Options: 'supervised', 'sac'\n",
    "\n",
    "# =============================================================================\n",
    "# SUPERVISED LEARNING CONFIGURATION\n",
    "# =============================================================================\n",
    "SUPERVISED_CONFIG = {\n",
    "    # Model architecture\n",
    "    \"hidden_size_lstm\": 128,\n",
    "    \"num_layers_lstm\": 2,\n",
    "    \"cnn_num_filters\": 64,\n",
    "    \"attention_dim\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 3,  # 3 for classification (down/neutral/up)\n",
    "    # Training hyperparameters\n",
    "    \"epochs\": 20,  # Reduced for Colab free tier time limits\n",
    "    \"batch_size\": 32,  # Lowered for RAM\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_amp\": True,  # Enable for 2x faster training\n",
    "    # Data splitting\n",
    "    \"train_ratio\": 0.7,\n",
    "    \"val_ratio\": 0.15,\n",
    "    \"test_ratio\": 0.15,\n",
    "    # Checkpointing\n",
    "    \"checkpoint_dir\": \"models/checkpoints\",\n",
    "    \"early_stop_patience\": 5,\n",
    "    \"lr_scheduler_patience\": 3,\n",
    "    \"lr_scheduler_factor\": 0.5,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# SAC (REINFORCEMENT LEARNING) CONFIGURATION\n",
    "# =============================================================================\n",
    "SAC_CONFIG = {\n",
    "    # Agent hyperparameters\n",
    "    \"hidden_dim\": 256,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"alpha\": 0.2,\n",
    "    \"auto_entropy_tuning\": True,\n",
    "    # Training configuration\n",
    "    \"total_steps\": 10000,  # Lowered for Colab\n",
    "    \"batch_size\": 128,\n",
    "    \"replay_buffer_size\": 10000,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"update_interval\": 1,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"eval_episodes\": 5,\n",
    "    # Environment configuration\n",
    "    \"initial_cash\": 50000.0,\n",
    "    \"time_horizon\": 100,\n",
    "    \"commission_pct\": 0.0001,\n",
    "    \"spread_bps\": 1.0,\n",
    "    \"reward_type\": \"incremental_pnl\",\n",
    "    \"reward_scaling\": 1.0,\n",
    "    # Checkpointing\n",
    "    \"checkpoint_dir\": \"models/sac_checkpoints\",\n",
    "    \"save_interval\": 5000,\n",
    "}\n",
    "\n",
    "print(f\"Training mode: {TRAINING_MODE.upper()}\")\n",
    "print(f\"Pairs to download: {PAIRS_TO_DOWNLOAD}\")\n",
    "print(f\"Years: {YEARS_TO_DOWNLOAD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb3631",
   "metadata": {},
   "source": [
    "## 6. GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcdd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU Environment Check\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    Compute Capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected. Training will use CPU.\")\n",
    "    print(\"For best performance: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b0c67",
   "metadata": {},
   "source": [
    "## 7. Data Collection from HistData\n",
    "\n",
    "This cell downloads historical FX data from HistData.com. It may take several minutes depending on the number of pairs and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da87a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from histdata.api import download_hist_data\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì• Downloading Historical FX Data from HistData.com\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Pairs: {', '.join([p.upper() for p in PAIRS_TO_DOWNLOAD])}\")\n",
    "print(f\"Years: {', '.join(YEARS_TO_DOWNLOAD)}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "raw_data_dir = ROOT / 'data' / 'raw'\n",
    "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_stats = {'successful': 0, 'failed': 0}\n",
    "\n",
    "for pair in PAIRS_TO_DOWNLOAD:\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"üìä Pair: {pair.upper()}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    pair_dir = raw_data_dir / pair\n",
    "    pair_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for year in YEARS_TO_DOWNLOAD:\n",
    "        try:\n",
    "            print(f\"  {year}... \", end='', flush=True)\n",
    "            \n",
    "            # Try downloading full year first\n",
    "            try:\n",
    "                result = download_hist_data(\n",
    "                    year=int(year),\n",
    "                    pair=pair,\n",
    "                    output_directory=str(pair_dir),\n",
    "                    verbose=False\n",
    "                )\n",
    "                print(f\"‚úì {result}\")\n",
    "                download_stats['successful'] += 1\n",
    "                \n",
    "            except AssertionError:\n",
    "                # Download month-by-month if full year not available\n",
    "                print(\"(month-by-month)\")\n",
    "                months_ok = 0\n",
    "                for month in range(1, 13):\n",
    "                    try:\n",
    "                        download_hist_data(\n",
    "                            year=int(year),\n",
    "                            month=month,\n",
    "                            pair=pair,\n",
    "                            output_directory=str(pair_dir),\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        months_ok += 1\n",
    "                        download_stats['successful'] += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                print(f\"    ‚úì {months_ok} months downloaded\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó {str(e)[:50]}\")\n",
    "            download_stats['failed'] += 1\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úì Successful: {download_stats['successful']}\")\n",
    "print(f\"‚úó Failed: {download_stats['failed']}\")\n",
    "print(\"‚úì Data collection complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb7331",
   "metadata": {},
   "source": [
    "## 8. Data Preparation\n",
    "\n",
    "This cell processes raw FX data and creates features for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Starting data preparation pipeline...\\n\")\n",
    "\n",
    "for pair in PAIRS_TO_DOWNLOAD:\n",
    "    print(f\"\\nProcessing {pair.upper()}...\")\n",
    "    \n",
    "    cmd = [\n",
    "        'python', str(ROOT / 'data' / 'prepare_dataset.py'),\n",
    "        '--pairs', pair,\n",
    "        '--t-in', str(T_IN),\n",
    "        '--t-out', str(T_OUT),\n",
    "        '--task-type', TASK_TYPE,\n",
    "    ]\n",
    "    \n",
    "    if USE_INTRINSIC_TIME:\n",
    "        cmd.append('--intrinsic-time')\n",
    "    if INCLUDE_SENTIMENT:\n",
    "        cmd.append('--include-sentiment')\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(ROOT))\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"  ‚úì {pair.upper()} prepared\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Failed: {result.stderr}\")\n",
    "\n",
    "print(\"\\n‚úì Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4bd14",
   "metadata": {},
   "source": [
    "## 9. Core Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Ensure paths are set\n",
    "ROOT = Path('/content/Sequence')\n",
    "sys.path.insert(0, str(ROOT))\n",
    "sys.path.insert(0, str(ROOT / 'run'))\n",
    "\n",
    "# Core imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project imports\n",
    "from config.config import ModelConfig\n",
    "from utils.multi_gpu import setup_multi_gpu\n",
    "from models.agent_hybrid import HybridCNNLSTMAttention\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844e3ff",
   "metadata": {},
   "source": [
    "## 10. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unwrapped_model(model):\n",
    "    \"\"\"Get the underlying model from DataParallel wrapper.\"\"\"\n",
    "    return model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into human-readable time.\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    \n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        return f\"{secs}s\"\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets, t_in):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.LongTensor(targets) if targets.dtype == np.int64 else torch.FloatTensor(targets)\n",
    "        self.t_in = t_in\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.t_in\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx + self.t_in]\n",
    "        y = self.targets[idx + self.t_in]\n",
    "        return x, y\n",
    "\n",
    "print(\"‚úì Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb46df",
   "metadata": {},
   "source": [
    "## 11. Supervised Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c55c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_supervised(model, loader, criterion, optimizer, device, scaler=None, grad_clip=None):\n",
    "    \"\"\"Train for one epoch (supervised learning).\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate_supervised(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model (supervised learning).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def train_supervised(pair, config):\n",
    "    \"\"\"Complete supervised training pipeline for a single pair.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SUPERVISED TRAINING: {pair.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    data_path = ROOT / 'data' / 'data' / pair / f'{pair}_prepared.csv'\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úì Loaded {len(df):,} rows\\n\")\n",
    "    \n",
    "    # Separate features and targets\n",
    "    target_col = 'target' if 'target' in df.columns else 'label'\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'timestamp', 'date', 'time']]\n",
    "    features = df[feature_cols].values\n",
    "    targets = df[target_col].values\n",
    "    \n",
    "    # Split data\n",
    "    n_samples = len(features)\n",
    "    train_end = int(n_samples * config['train_ratio'])\n",
    "    val_end = int(n_samples * (config['train_ratio'] + config['val_ratio']))\n",
    "    \n",
    "    train_features, train_targets = features[:train_end], targets[:train_end]\n",
    "    val_features, val_targets = features[train_end:val_end], targets[train_end:val_end]\n",
    "    test_features, test_targets = features[val_end:], targets[val_end:]\n",
    "    \n",
    "    print(\"Dataset split:\")\n",
    "    print(f\"  Train: {len(train_features):,} samples ({config['train_ratio']*100:.0f}%)\")\n",
    "    print(f\"  Val:   {len(val_features):,} samples ({config['val_ratio']*100:.0f}%)\")\n",
    "    print(f\"  Test:  {len(test_features):,} samples ({config['test_ratio']*100:.0f}%)\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SequenceDataset(train_features, train_targets, T_IN)\n",
    "    val_dataset = SequenceDataset(val_features, val_targets, T_IN)\n",
    "    test_dataset = SequenceDataset(test_features, test_targets, T_IN)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # Create model\n",
    "    model_config = ModelConfig(\n",
    "        num_features=len(feature_cols),\n",
    "        hidden_size_lstm=config['hidden_size_lstm'],\n",
    "        num_layers_lstm=config['num_layers_lstm'],\n",
    "        cnn_num_filters=config['cnn_num_filters'],\n",
    "        attention_dim=config['attention_dim'],\n",
    "        dropout=config['dropout'],\n",
    "        num_classes=config['num_classes'],\n",
    "        use_optimized_attention=False\n",
    "    )\n",
    "    \n",
    "    model = HybridCNNLSTMAttention(model_config)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, device = setup_multi_gpu(model, device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: HybridCNNLSTMAttention ({total_params:,} parameters)\")\n",
    "    print(f\"Device: {device}\\n\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=config['lr_scheduler_factor'],\n",
    "        patience=config['lr_scheduler_patience'], verbose=False\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler() if config['use_amp'] and torch.cuda.is_available() else None\n",
    "    \n",
    "    # Checkpoint directory\n",
    "    checkpoint_dir = ROOT / config['checkpoint_dir']\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f'{pair}_best_model.pt'\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Starting training for {config['epochs']} epochs...\\n\")\n",
    "    \n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_acc = train_epoch_supervised(\n",
    "            model, train_loader, criterion, optimizer, device, scaler, config['grad_clip']\n",
    "        )\n",
    "        val_loss, val_acc = validate_supervised(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{config['epochs']} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% | \"\n",
    "                  f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': get_unwrapped_model(model).state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'model_config': model_config.__dict__,\n",
    "                'pair': pair,\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= config['early_stop_patience']:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Test evaluation\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    get_unwrapped_model(model).load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_loss, test_acc = validate_supervised(model, test_loader, criterion, device)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING COMPLETE: {pair.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Time elapsed: {format_time(elapsed_time)}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.4f} (epoch {checkpoint['epoch']})\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Checkpoint: {checkpoint_path}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'pair': pair,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'epochs_trained': len(history['train_loss']),\n",
    "        'time_elapsed': elapsed_time,\n",
    "        'checkpoint_path': str(checkpoint_path)\n",
    "    }\n",
    "\n",
    "print(\"‚úì Supervised learning functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96341e9",
   "metadata": {},
   "source": [
    "## 12. Multi-Pair Training Loop\n",
    "\n",
    "This cell trains models sequentially for all configured pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2415fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results for all pairs\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"MULTI-PAIR TRAINING SESSION\")\n",
    "print(f\"{'#'*80}\")\n",
    "print(f\"Mode: {TRAINING_MODE.upper()}\")\n",
    "print(f\"Pairs: {', '.join([p.upper() for p in PAIRS_TO_DOWNLOAD])}\")\n",
    "print(f\"Total pairs: {len(PAIRS_TO_DOWNLOAD)}\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "session_start_time = time.time()\n",
    "\n",
    "for idx, pair in enumerate(PAIRS_TO_DOWNLOAD, 1):\n",
    "    print(f\"\\n[{idx}/{len(PAIRS_TO_DOWNLOAD)}] Processing {pair.upper()}...\")\n",
    "    \n",
    "    try:\n",
    "        if TRAINING_MODE == 'supervised':\n",
    "            result = train_supervised(pair, SUPERVISED_CONFIG)\n",
    "        elif TRAINING_MODE == 'sac':\n",
    "            print(\"‚ö†Ô∏è  SAC training not implemented in this notebook yet.\")\n",
    "            print(\"   For SAC, use the RL training scripts or add SAC functions.\")\n",
    "            result = {'pair': pair, 'error': 'SAC not implemented'}\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown training mode: {TRAINING_MODE}\")\n",
    "        \n",
    "        all_results.append(result)\n",
    "        print(f\"‚úì {pair.upper()} completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {pair.upper()} failed: {e}\")\n",
    "        all_results.append({\n",
    "            'pair': pair,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Show remaining pairs\n",
    "    if idx < len(PAIRS_TO_DOWNLOAD):\n",
    "        remaining = PAIRS_TO_DOWNLOAD[idx:]\n",
    "        print(f\"\\nRemaining pairs: {', '.join([p.upper() for p in remaining])}\")\n",
    "\n",
    "session_elapsed_time = time.time() - session_start_time\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"ALL TRAINING COMPLETE\")\n",
    "print(f\"{'#'*80}\")\n",
    "print(f\"Total session time: {format_time(session_elapsed_time)}\")\n",
    "print(f\"Pairs processed: {len(all_results)}/{len(PAIRS_TO_DOWNLOAD)}\")\n",
    "print(f\"{'#'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab754ca",
   "metadata": {},
   "source": [
    "## 13. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e92e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE == 'supervised':\n",
    "    # Plot test accuracy comparison\n",
    "    successful_results = [r for r in all_results if 'test_acc' in r]\n",
    "    \n",
    "    if successful_results:\n",
    "        pairs = [r['pair'].upper() for r in successful_results]\n",
    "        test_accs = [r['test_acc'] for r in successful_results]\n",
    "        times = [r['time_elapsed'] / 60 for r in successful_results]  # Convert to minutes\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Test accuracy\n",
    "        axes[0].bar(pairs, test_accs, color='steelblue', alpha=0.8)\n",
    "        axes[0].set_ylabel('Test Accuracy (%)')\n",
    "        axes[0].set_title('Test Accuracy by Pair')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[0].axhline(y=50, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Training time\n",
    "        axes[1].bar(pairs, times, color='coral', alpha=0.8)\n",
    "        axes[1].set_ylabel('Training Time (minutes)')\n",
    "        axes[1].set_title('Training Time by Pair')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        for r in successful_results:\n",
    "            print(f\"{r['pair'].upper():8s} | Acc: {r['test_acc']:6.2f}% | Time: {r['time_elapsed']/60:5.1f}m\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"No successful results to visualize.\")\n",
    "elif TRAINING_MODE == 'sac':\n",
    "    print(\"SAC visualization not implemented. Add SAC evaluation metrics for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db312dc3",
   "metadata": {},
   "source": [
    "## 14. Backup Checkpoints to Google Drive\n",
    "\n",
    "This ensures your trained models persist even if the Colab session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "drive_checkpoint_dir = Path('/content/drive/MyDrive/Sequence_Models')\n",
    "drive_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Backing up checkpoints to Google Drive...\\n\")\n",
    "\n",
    "checkpoint_dir = ROOT / 'models' / 'checkpoints'\n",
    "backup_count = 0\n",
    "\n",
    "for checkpoint_file in checkpoint_dir.glob('*.pt'):\n",
    "    shutil.copy2(checkpoint_file, drive_checkpoint_dir / checkpoint_file.name)\n",
    "    print(f\"‚úì {checkpoint_file.name}\")\n",
    "    backup_count += 1\n",
    "\n",
    "print(f\"\\n‚úì {backup_count} checkpoint(s) saved to: {drive_checkpoint_dir}\")\n",
    "print(\"\\nYour models are now safely backed up to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63383b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Download Checkpoints**: Access them from your Google Drive at `MyDrive/Sequence_Models/`\n",
    "2. **Extend Training**: Add more pairs to `PAIRS_TO_DOWNLOAD` and re-run\n",
    "3. **Evaluate**: Use the checkpoints for backtesting and live trading\n",
    "4. **Experiment**: Try different hyperparameters or enable intrinsic time\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **Out of memory**: Reduce `batch_size` or number of pairs\n",
    "- **Session timeout**: Save checkpoints more frequently\n",
    "- **Import errors**: Re-run the setup cells\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Main documentation: `CLAUDE.md` in repository\n",
    "- Colab guide: `notebooks/COLAB_SETUP_GUIDE.md`\n",
    "- Training scripts: `run/training_pipeline.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
