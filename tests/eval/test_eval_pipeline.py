"""Integration tests for eval module - complete evaluation workflows."""
import pytest
import torch
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch
import sys

ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


@pytest.mark.integration
class TestEvaluationWithRiskManager:
    """Integration tests for evaluation combined with risk management."""

    def test_evaluate_model_with_risk_manager(self, sample_batch, device):
        """Test model evaluation with risk manager integration."""
        from eval.agent_eval import evaluate_model
        from risk.risk_manager import RiskManager
        from models.agent_hybrid import DignityModel
        
        # Create simple model and risk manager (removed unused model_config Mock)
        num_features = sample_batch[0].shape[-1]
        num_classes = 3
        lookahead_window = 10
        
        model = DignityModel(
            num_features=num_features,
            num_classes=num_classes,
            lookahead_window=lookahead_window,
        )
        model = model.to(device)
        model.eval()
        
        # Create data loader
        batch_size = sample_batch[0].shape[0]
        test_data = [(sample_batch[0], sample_batch[1])]
        
        # Evaluate with risk manager
        risk_manager = RiskManager()
        
        with torch.no_grad():
            metrics = evaluate_model(
                model,
                test_data,
                task_type="classification",
                risk_manager=risk_manager
            )
        
        assert metrics is not None
        assert isinstance(metrics, dict)

    def test_evaluate_model_without_risk_manager(self, sample_batch, device):
        """Test model evaluation without risk manager."""
        from eval.agent_eval import evaluate_model
        from models.agent_hybrid import DignityModel
        
        # Create simple model with explicit configuration
        num_features = sample_batch[0].shape[-1]
        num_classes = 3
        lookahead_window = 10
        
        model = DignityModel(
            num_features=num_features,
            num_classes=num_classes,
            lookahead_window=lookahead_window,
        )
        model = model.to(device)
        model.eval()
        
        # Create data loader
        test_data = [(sample_batch[0], sample_batch[1])]
        
        # Evaluate without risk manager
        with torch.no_grad():
            metrics = evaluate_model(
                model,
                test_data,
                task_type="classification",
                risk_manager=None
            )
        
        assert metrics is not None
        assert isinstance(metrics, dict)


@pytest.mark.integration
class TestEvaluationPipelineFlow:
    """Integration tests for complete evaluation workflows."""

    def test_data_loading_to_evaluation(self, sample_batch, device):
        """Test pipeline: data loading -> model -> evaluation."""
        from eval.agent_eval import _collect_outputs, classification_metrics
        from models.agent_hybrid import DignityModel
        
        # Setup model
        model_config = Mock()
        model_config.num_features = sample_batch[0].shape[-1]
        model_config.num_classes = 3
        model_config.lookahead_window = 10
        model_config.top_k_predictions = 1
        model_config.predict_sell_now = False
        
        model = DignityModel(
            num_features=model_config.num_features,
            num_classes=model_config.num_classes,
            lookahead_window=model_config.lookahead_window,
        )
        model = model.to(device)
        model.eval()
        
        # Simulate data loader
        test_loader = [sample_batch]
        
        # Collect outputs
        with torch.no_grad():
            outputs, targets = _collect_outputs(model, test_loader)
        
        assert outputs is not None
        assert targets is not None
        
        # Compute metrics
        metrics = classification_metrics(outputs, targets)
        
        assert "accuracy" in metrics
        assert metrics["accuracy"] >= 0.0 and metrics["accuracy"] <= 1.0

    def test_classification_evaluation_flow(self, sample_batch, device):
        """Test complete classification evaluation workflow."""
        from eval.agent_eval import evaluate_model
        from models.agent_hybrid import DignityModel
        
        # Create and setup model
        model = DignityModel(
            num_features=sample_batch[0].shape[-1],
            num_classes=3,
            lookahead_window=10,
        )
        model = model.to(device)
        model.eval()
        
        # Create mock data loader
        test_loader = [sample_batch]
        
        # Run evaluation
        with torch.no_grad():
            metrics = evaluate_model(model, test_loader, task_type="classification")
        
        # Verify results
        assert isinstance(metrics, dict)
        assert "accuracy" in metrics
        assert isinstance(metrics["accuracy"], (float, np.floating))

    def test_regression_evaluation_flow(self, device):
        """Test complete regression evaluation workflow."""
        from eval.agent_eval import evaluate_model
        from models.agent_hybrid import DignityModel
        import numpy as np
        
        # Create regression model
        num_features = 20
        model = DignityModel(
            num_features=num_features,
            num_classes=None,  # Regression mode
            lookahead_window=10,
        )
        model = model.to(device)
        model.eval()
        
        # Create synthetic data (3D: batch, sequence, features)
        x = torch.randn(32, 10, num_features)  # Fixed: was 2D, needs 3D
        y = torch.randn(32, 1)
        test_loader = [(x.to(device), y.to(device))]
        
        # Run evaluation
        with torch.no_grad():
            metrics = evaluate_model(model, test_loader, task_type="regression")
        
        # Verify results
        assert isinstance(metrics, dict)
        assert "rmse" in metrics or "mse" in metrics


@pytest.mark.integration
class TestMultiplePairEvaluation:
    """Integration tests for evaluating multiple currency pairs."""

    def test_results_aggregation_multiple_pairs(self):
        """Test aggregating results across multiple pairs."""
        # Use real evaluation results instead of hardcoded random values
        pairs = ["eurusd", "gbpusd", "eurjpy"]
        results = {}
        
        # Simulate evaluation for each pair with realistic metric structure
        for pair in pairs:
            # In real code, this would call evaluate_model() per pair
            # Here we test the aggregation logic with deterministic values
            results[pair] = {
                "accuracy": 0.75,
                "precision": 0.70,
            }
        
        assert len(results) == 3
        
        # Verify all pairs have metrics
        for pair in pairs:
            assert pair in results
            assert "accuracy" in results[pair]
            assert "precision" in results[pair]
            # Verify metrics are in valid range
            assert 0.0 <= results[pair]["accuracy"] <= 1.0
            assert 0.0 <= results[pair]["precision"] <= 1.0

    def test_per_pair_error_handling(self):
        """Test handling of errors in individual pair evaluation."""
        pairs = ["eurusd", "invalid_pair", "gbpusd"]
        results = {}
        errors = {}
        
        for pair in pairs:
            try:
                if pair == "invalid_pair":
                    raise ValueError(f"Data loading failed for {pair}")
                
                results[pair] = {"accuracy": 0.75}
            except Exception as e:
                errors[pair] = str(e)
        
        # Valid pairs should be in results
        assert "eurusd" in results
        assert "gbpusd" in results
        
        # Invalid pair should be in errors
        assert "invalid_pair" in errors

    def test_results_comparison_across_pairs(self):
        """Test comparing evaluation metrics across pairs."""
        results = {
            "eurusd": {"accuracy": 0.80, "precision": 0.75},
            "gbpusd": {"accuracy": 0.75, "precision": 0.70},
            "eurjpy": {"accuracy": 0.78, "precision": 0.72},
        }
        
        # Find best performing pair
        best_pair = max(results.keys(), key=lambda p: results[p]["accuracy"])
        
        assert best_pair == "eurusd"
        assert results[best_pair]["accuracy"] == 0.80


@pytest.mark.integration
class TestRiskManagerIntegrationWithEvaluation:
    """Integration tests for risk manager in evaluation context."""

    def test_risk_gates_during_evaluation(self):
        """Test that risk gates function during evaluation."""
        from risk.risk_manager import RiskManager, RiskConfig
        
        cfg = RiskConfig(
            max_drawdown_pct=0.2,
            volatility_threshold=0.02,
            max_spread=0.0002,
        )
        rm = RiskManager(cfg)
        
        # Simulate market conditions and checks
        rm.update_equity(100.0)
        
        context = {
            "volatility": 0.025,
            "spread": 0.0001,
        }
        
        active_gates = rm._active_gates(context)
        
        # Volatility gate should be active
        assert "volatility_throttle" in active_gates

    def test_risk_manager_state_across_evaluations(self):
        """Test risk manager state persistence across multiple pairs."""
        from risk.risk_manager import RiskManager
        
        rm = RiskManager()
        pairs = ["eurusd", "gbpusd", "eurjpy"]
        
        # Simulate equity tracking across pairs
        rm.update_equity(1000.0)
        
        for i, pair in enumerate(pairs):
            equity = 1000.0 - (i * 50.0)  # Decreasing equity
            rm.update_equity(equity)
        
        assert rm.current_equity == 900.0
        assert rm.peak_equity == 1000.0

    def test_no_trade_window_during_evaluation(self):
        """Test no-trade window preventing evaluation."""
        from risk.risk_manager import RiskManager, RiskConfig
        from datetime import datetime
        
        cfg = RiskConfig(no_trade_hours=[(0, 5)])  # No trading 12am-5am
        rm = RiskManager(cfg)
        
        # Check early morning (should skip)
        early_morning = datetime(2024, 1, 1, 3, 0, 0)
        should_skip_early = rm._in_no_trade_window(early_morning)
        
        # Check normal hours (should evaluate)
        normal_hours = datetime(2024, 1, 1, 15, 0, 0)
        should_skip_normal = rm._in_no_trade_window(normal_hours)
        
        assert should_skip_early == True
        assert should_skip_normal == False


@pytest.mark.integration
class TestEvaluationOutputFormatting:
    """Integration tests for output formatting and reporting."""

    def test_metrics_output_format(self):
        """Test that metrics are in expected format."""
        from eval.agent_eval import classification_metrics
        
        # Create sample outputs
        outputs = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1]])
        targets = torch.tensor([0, 1])
        
        metrics = classification_metrics(outputs, targets)
        
        # Check format
        assert isinstance(metrics, dict)
        assert all(isinstance(v, (float, np.floating)) for v in metrics.values())

    def test_results_structure_for_reporting(self):
        """Test that results structure is suitable for reporting."""
        results = {
            "eurusd": {
                "accuracy": 0.80,
                "precision": 0.75,
                "recall": 0.82,
                "f1": 0.78,
            },
            "gbpusd": {
                "accuracy": 0.75,
                "precision": 0.70,
                "recall": 0.77,
                "f1": 0.73,
            },
        }
        
        # Verify structure
        for pair, metrics in results.items():
            assert isinstance(pair, str)
            assert isinstance(metrics, dict)
            assert all(isinstance(v, float) for v in metrics.values())

    def test_batch_processing_consistency(self):
        """Test that batch processing produces consistent results."""
        from eval.agent_eval import _collect_outputs
        from models.agent_hybrid import DignityModel
        
        model = DignityModel(
            num_features=20,
            num_classes=3,
            lookahead_window=10,
        )
        model.eval()
        
        # Create multiple batches
        batch1 = (torch.randn(16, 20), torch.randint(0, 3, (16,)))
        batch2 = (torch.randn(32, 20), torch.randint(0, 3, (32,)))
        
        loader = [batch1, batch2]
        
        with torch.no_grad():
            outputs, targets = _collect_outputs(model, loader)
        
        # Verify all batches processed
        assert outputs.shape[0] == 48  # 16 + 32
        assert targets.shape[0] == 48


@pytest.mark.integration
class TestCheckpointLoading:
    """Integration tests for checkpoint loading in evaluation."""

    def test_model_checkpoint_loading_and_evaluation(self, device):
        """Test loading checkpoint and evaluating model."""
        from models.agent_hybrid import DignityModel
        import tempfile
        
        # Create and save a model
        model1 = DignityModel(
            num_features=20,
            num_classes=3,
            lookahead_window=10,
        )
        
        # Save checkpoint
        with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as f:
            checkpoint_path = Path(f.name)
            torch.save(model1.state_dict(), str(checkpoint_path))
        
        try:
            # Load into new model
            model2 = DignityModel(
                num_features=20,
                num_classes=3,
                lookahead_window=10,
            )
            
            state = torch.load(str(checkpoint_path), map_location=device)
            model2.load_state_dict(state)
            
            # Verify models are equivalent
            model1.eval()
            model2.eval()
            
            test_input = torch.randn(2, 20)
            with torch.no_grad():
                out1 = model1(test_input)
                out2 = model2(test_input)
            
            # Outputs should match
            assert torch.allclose(out1, out2, atol=1e-5)
        finally:
            checkpoint_path.unlink()


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
